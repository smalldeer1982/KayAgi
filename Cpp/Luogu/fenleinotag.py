import os
import re
from collections import Counter

def extract_tags_and_content(file_path):
    """
    ä» Markdown æ–‡ä»¶ä¸­æå–æ ‡ç­¾(tag)å’Œæ­£æ–‡å†…å®¹ï¼Œå¹¶è§£æé¢˜å·ã€‚
    """
    with open(file_path, "r", encoding="utf-8") as file:
        content = file.read()

        # æå–æ ‡ç­¾ï¼Œæ”¯æŒ ['åŠ¨æ€è§„åˆ’ DP'] æ ¼å¼
        tag_match = re.search(r"tag:\s*\[(.*?)\]", content)
        if tag_match:
            raw_tags = tag_match.group(1)
            # **æ‹†åˆ†æ—¶è€ƒè™‘å«ç©ºæ ¼çš„ tag**
            tags = [t.strip().strip("'\"") for t in re.split(r",\s*", raw_tags)]
        else:
            tags = []

        # æå–é¢˜å·
        title_match = re.search(r"title:\s*\"?(.+?)\"?\n", content)
        pid_match = re.search(r"pid:\s*(.+)", content)

        title = title_match.group(1).strip() if title_match else "æœªçŸ¥é¢˜å·"
        pid = pid_match.group(1).strip() if pid_match else "æœªçŸ¥PID"

        print(f"ğŸ“œ è§£ææ–‡ä»¶: {file_path}")
        print(f"ğŸ” æå–æ ‡ç­¾: {tags}")
        print(f"ğŸ”¢ é¢˜å·: {pid}ï¼ˆ{title}ï¼‰\n")

        return tags, pid, title, content


def consolidate_files_by_tag(target_dir, output_base_dir, target_tag):
    """
    æŒ‰ç…§æŒ‡å®šæ ‡ç­¾ï¼ˆtarget_tagï¼‰æ£€ç´¢å¹¶æ‹¼æ¥ Markdown æ–‡ä»¶ï¼Œå¹¶è¾“å‡ºé¢˜å·åˆ—è¡¨ã€‚
    """
    matched_contents = []
    matched_titles = []

    diff_name = os.path.basename(target_dir)

    for file_name in os.listdir(target_dir):
        if file_name.endswith(".md"):
            file_path = os.path.join(target_dir, file_name)
            tags, pid, title, content = extract_tags_and_content(file_path)

            # âœ… **å…è®¸éƒ¨åˆ†åŒ¹é…**
            if any(target_tag in tag for tag in tags):
                matched_contents.append(content)
                matched_titles.append(f"{pid} - {title}")

    if matched_contents:
        output_dir = os.path.join(output_base_dir, diff_name)
        os.makedirs(output_dir, exist_ok=True)

        # ä¿å­˜æ•´åˆçš„ Markdown æ–‡ä»¶
        output_file = os.path.join(output_dir, f"consolidated_{diff_name}_{target_tag}.md")
        with open(output_file, "w", encoding="utf-8") as output:
            for content in matched_contents:
                output.write(content + "\n\n---\n\n")
        print(f"âœ… æ•´åˆå®Œæˆ: {output_file}")

        # ç”Ÿæˆé¢˜å·åˆ—è¡¨
        list_output_file = os.path.join(output_dir, f"consolidated_{diff_name}_{target_tag}_list.md")
        with open(list_output_file, "w", encoding="utf-8") as list_output:
            list_output.write("# é¢˜å·åˆ—è¡¨\n\n")
            for title in matched_titles:
                list_output.write(f"- {title}\n")
        print(f"âœ… é¢˜å·åˆ—è¡¨å·²ä¿å­˜: {list_output_file}")
    else:
        print(f"âš ï¸ æœªæ‰¾åˆ°åŒ…å«æ ‡ç­¾ '{target_tag}' çš„æ–‡ä»¶ã€‚")

def process_tags(tags):
    """å¤„ç†æ ‡ç­¾åˆ—è¡¨ï¼Œè¿”å›æ ¼å¼åŒ–çš„å­—ç¬¦ä¸²"""
    if not tags:
        return ""
    return ", ".join(tags)  # åªåœ¨æ ‡ç­¾ä¹‹é—´æ·»åŠ ", "åˆ†éš”ç¬¦

def find_files_without_tags(target_dir, output_base_dir):
    """æŸ¥æ‰¾æ²¡æœ‰æ ‡ç­¾çš„é¢˜ç›®æ–‡ä»¶ï¼Œå¹¶æ•´åˆåˆ°ä¸€ä¸ªæ–‡ä»¶ä¸­"""
    no_tag_contents = []
    no_tag_titles = []
    analysis_results = []  # ç”¨äºå­˜å‚¨åˆ†æç»“æœ
    
    diff_name = os.path.basename(target_dir)
    
    excluded_tags = [
                'USACO', 'O2ä¼˜åŒ–', 'Special Judge', 'ç¦å»ºçœå†å±Šå¤ä»¤è¥', 'è¯­è¨€æœˆèµ›', 'æ´›è°·æœˆèµ›', 'æ´›è°·åŸåˆ›',
                'COCI', 'NOIP æ™®åŠç»„', 'è“æ¡¥æ¯çœèµ›', 'ICPC', 'æ±Ÿè‹', 'ä¿¡æ¯ä¸æœªæ¥', 'NOIP æé«˜ç»„', 'CCC', 'ROIR',
                'ä¼ æ™ºæ¯', 'è“æ¡¥æ¯å›½èµ›', 'æ¢¦ç†Šæ¯”èµ›', 'å„çœçœé€‰', 'å±±ä¸œ', 'PA', 'CSP-X å°å­¦ç»„', 'é«˜æ ¡æ ¡èµ›', 'NOI å¯¼åˆŠ',
                'å®‰å¾½', 'åˆ†ç±»è®¨è®º', 'CSP-J å…¥é—¨çº§', 'æäº¤ç­”æ¡ˆ', 'XCPC', 'JOI', 'Ad-hoc', 'NOISG', 'CSP-S æé«˜çº§',
                'POI', 'åŒ—äº¬', 'é™•è¥¿', 'Code+', 'å¹¿ä¸œ', 'BalticOI', 'NOI Online', 'BCSP-X', 'æ´›è°·æ¯”èµ›', 'IOI',
                'NOI', 'å¤©æ´¥', 'äº¤äº’é¢˜', '\ufeffåŸºç¡€ç®—æ³•', 'å—äº¬', 'ç¦å»º', 'å°å­¦ç§‘åˆ›æ´»åŠ¨', 'æ¹–åŒ—', 'æ¹–å—', 'æ²³å—',
                'é›†è®­é˜Ÿäº’æµ‹', 'å‰æ—', 'é€†å…ƒ', 'äº‘å—', 'CEOI', 'æ¾³é—¨', 'æµå—', 'ä¸Šæµ·', 'é’å²›', 'GESP',
                'JOIï¼ˆæ—¥æœ¬ï¼‰', 'BalticOIï¼ˆæ³¢ç½—çš„æµ·ï¼‰', 'CEOIï¼ˆä¸­æ¬§ï¼‰', 'COCIï¼ˆå…‹ç½—åœ°äºšï¼‰', 
                'CCCï¼ˆåŠ æ‹¿å¤§ï¼‰', 'PAï¼ˆæ³¢å…°ï¼‰', 'POIï¼ˆæ³¢å…°ï¼‰', 'ROIRï¼ˆä¿„ç½—æ–¯ï¼‰','NOISGï¼ˆæ–°åŠ å¡ï¼‰'
    ]
    
    for file_name in os.listdir(target_dir):
        if file_name.endswith(".md"):
            file_path = os.path.join(target_dir, file_name)
            with open(file_path, "r", encoding="utf-8") as file:
                content = file.read()
                
                # æå–æ ‡ç­¾
                tag_match = re.search(r"tag:\s*\[(.*?)\]", content)
                tags = []
                is_empty_tag = False
                
                if tag_match:
                    raw_tags = tag_match.group(1)
                    if raw_tags.strip():  # æœ‰éç©ºæ ‡ç­¾
                        tags = [t.strip().strip("'\"") for t in re.split(r",\s*", raw_tags)]
                    else:
                        is_empty_tag = True
                
                # æå–é¢˜å·å’Œæ ‡é¢˜
                title_match = re.search(r"title:\s*\"?(.+?)\"?\n", content)
                pid_match = re.search(r"pid:\s*(.+)", content)
                
                title = title_match.group(1).strip() if title_match else "æœªçŸ¥é¢˜å·"
                pid = pid_match.group(1).strip() if pid_match else "æœªçŸ¥PID"
                
                # è¿‡æ»¤æ ‡ç­¾
                filtered_tags = [tag for tag in tags 
                               if tag not in excluded_tags 
                               and not tag.isdigit() 
                               and not (tag.startswith('20') and len(tag) == 4)]
                
                # åˆ¤æ–­æ˜¯å¦ä¸ºæ— æ ‡ç­¾é¢˜ç›®
                if not filtered_tags or is_empty_tag:
                    no_tag_contents.append(content)
                    no_tag_titles.append(f"- {pid} - {title}")
                
                # æ·»åŠ åˆ°åˆ†æç»“æœ
                analysis_results.append({
                    "pid": pid,
                    "original_tags": process_tags(tags),
                    "filtered_tags": process_tags(filtered_tags),
                    "is_empty": "æ˜¯" if not filtered_tags else "å¦",
                    "is_original_empty": "æ˜¯" if is_empty_tag or not tags else "å¦"
                })
    
    # å†™å…¥åˆ†æç»“æœ
    analysis_file = os.path.join(output_base_dir, "tag_analysis.txt")
    with open(analysis_file, "w", encoding="utf-8") as f:
        f.write("é¢˜å·\tåŸå§‹æ ‡ç­¾\tå¤„ç†åæ ‡ç­¾\tæ˜¯å¦ä¸ºç©ºæ ‡ç­¾\tåŸå§‹æ˜¯å¦ä¸ºç©º\n")
        for result in analysis_results:
            f.write(f"{result['pid']}\t{result['original_tags']}\t{result['filtered_tags']}\t{result['is_empty']}\t{result['is_original_empty']}\n")
    
    # å¤„ç†æ— æ ‡ç­¾é¢˜ç›®çš„è¾“å‡º
    if no_tag_contents:
        output_dir = os.path.join(output_base_dir, diff_name)
        os.makedirs(output_dir, exist_ok=True)
        
        # ä¿å­˜æ— æ ‡ç­¾é¢˜ç›®å†…å®¹
        output_file = os.path.join(output_dir, f"consolidated_{diff_name}_no_tag.md")
        with open(output_file, "w", encoding="utf-8") as output:
            for content in no_tag_contents:
                output.write(content + "\n\n---\n\n")
        print(f"âœ… æ— æ ‡ç­¾é¢˜ç›®æ•´åˆå®Œæˆ: {output_file}")
        
        # ä¿å­˜æ— æ ‡ç­¾é¢˜ç›®åˆ—è¡¨
        list_output_file = os.path.join(output_dir, f"consolidated_{diff_name}_no_tag_list.md")
        with open(list_output_file, "w", encoding="utf-8") as list_output:
            list_output.write("# æ— æ ‡ç­¾é¢˜ç›®åˆ—è¡¨\n\n")
            for title in sorted(no_tag_titles):  # æ’åºé¢˜ç›®åˆ—è¡¨
                list_output.write(f"{title}\n")
        print(f"âœ… æ— æ ‡ç­¾é¢˜ç›®åˆ—è¡¨å·²ä¿å­˜: {list_output_file}")
        print(f"ğŸ“Š å…±æ‰¾åˆ° {len(no_tag_titles)} ä¸ªæ— æ ‡ç­¾é¢˜ç›®")
    else:
        print(f"âš ï¸ æœªæ‰¾åˆ°æ— æ ‡ç­¾çš„é¢˜ç›®æ–‡ä»¶ã€‚")

def combine_all_md_files(output_base_dir, combined_file_name="combined.md"):
    """
    å°† OUTPUT_BASE_DIR æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰ Markdown æ–‡ä»¶åˆå¹¶ä¸ºä¸€ä¸ªæ–‡ä»¶ã€‚
    """
    combined_file_path = os.path.join(output_base_dir, combined_file_name)
    os.makedirs(output_base_dir, exist_ok=True)

    with open(combined_file_path, "w", encoding="utf-8") as combined_file:
        for root, _, files in os.walk(output_base_dir):
            for file_name in files:
                if file_name.endswith(".md"):
                    file_path = os.path.join(root, file_name)
                    with open(file_path, "r", encoding="utf-8") as file:
                        combined_file.write(file.read())
                        combined_file.write("\n\n---\n\n")
    print(f"âœ… æ‰€æœ‰ Markdown æ–‡ä»¶å·²åˆå¹¶åˆ°: {combined_file_path}")

def analyze_tag_statistics(target_dir):
    """
    åˆ†æç›®å½•ä¸­æ‰€æœ‰é¢˜ç›®çš„æ ‡ç­¾ç»Ÿè®¡ä¿¡æ¯
    """
    all_tags = []
    
    for file_name in os.listdir(target_dir):
        if file_name.endswith(".md"):
            file_path = os.path.join(target_dir, file_name)
            tags, _, _, _ = extract_tags_and_content(file_path)
            
            # è¿‡æ»¤æ‰éœ€è¦æ’é™¤çš„æ ‡ç­¾å’Œçº¯æ•°å­—æ ‡ç­¾
            excluded_tags = [
                'USACO', 'O2ä¼˜åŒ–', 'Special Judge', 'ç¦å»ºçœå†å±Šå¤ä»¤è¥', 'è¯­è¨€æœˆèµ›', 'æ´›è°·æœˆèµ›', 'æ´›è°·åŸåˆ›',
                'COCI', 'NOIP æ™®åŠç»„', 'è“æ¡¥æ¯çœèµ›', 'ICPC', 'æ±Ÿè‹', 'ä¿¡æ¯ä¸æœªæ¥', 'NOIP æé«˜ç»„', 'CCC', 'ROIR',
                'ä¼ æ™ºæ¯', 'è“æ¡¥æ¯å›½èµ›', 'æ¢¦ç†Šæ¯”èµ›', 'å„çœçœé€‰', 'å±±ä¸œ', 'PA', 'CSP-X å°å­¦ç»„', 'é«˜æ ¡æ ¡èµ›', 'NOI å¯¼åˆŠ',
                'å®‰å¾½', 'åˆ†ç±»è®¨è®º', 'CSP-J å…¥é—¨çº§', 'æäº¤ç­”æ¡ˆ', 'XCPC', 'JOI', 'Ad-hoc', 'NOISG', 'CSP-S æé«˜çº§',
                'POI', 'åŒ—äº¬', 'é™•è¥¿', 'Code+', 'å¹¿ä¸œ', 'BalticOI', 'NOI Online', 'BCSP-X', 'æ´›è°·æ¯”èµ›', 'IOI',
                'NOI', 'å¤©æ´¥', 'äº¤äº’é¢˜', '\ufeffåŸºç¡€ç®—æ³•', 'å—äº¬', 'ç¦å»º', 'å°å­¦ç§‘åˆ›æ´»åŠ¨', 'æ¹–åŒ—', 'æ¹–å—', 'æ²³å—',
                'é›†è®­é˜Ÿäº’æµ‹', 'å‰æ—', 'é€†å…ƒ', 'äº‘å—', 'CEOI', 'æ¾³é—¨', 'æµå—', 'ä¸Šæµ·', 'é’å²›', 'GESP',
                'JOIï¼ˆæ—¥æœ¬ï¼‰', 'BalticOIï¼ˆæ³¢ç½—çš„æµ·ï¼‰', 'CEOIï¼ˆä¸­æ¬§ï¼‰', 'COCIï¼ˆå…‹ç½—åœ°äºšï¼‰', 
                'CCCï¼ˆåŠ æ‹¿å¤§ï¼‰', 'PAï¼ˆæ³¢å…°ï¼‰', 'POIï¼ˆæ³¢å…°ï¼‰', 'ROIRï¼ˆä¿„ç½—æ–¯ï¼‰','NOISGï¼ˆæ–°åŠ å¡ï¼‰'
            ]
            filtered_tags = [tag for tag in tags 
                           if tag not in excluded_tags 
                           and not tag.isdigit() 
                           and not (tag.startswith('20') and len(tag) == 4)]
            
            all_tags.extend(filtered_tags)
    
    # ç»Ÿè®¡æ ‡ç­¾é¢‘ç‡
    tag_counter = Counter(all_tags)
    
    if tag_counter:
        total_tags = sum(tag_counter.values())
        unique_tags = len(tag_counter)
        
        print(f"\nğŸ“Š æ ‡ç­¾ç»Ÿè®¡ (å…±{unique_tags}ç§ä¸åŒæ ‡ç­¾ï¼Œæ€»è®¡{total_tags}ä¸ªæ ‡ç­¾)")
        print("=" * 80)
        print("æ’å  æ ‡ç­¾åç§°                 å‡ºç°æ¬¡æ•°  å æ¯”")
        print("-" * 80)
        
        for rank, (tag, count) in enumerate(tag_counter.most_common(), 1):
            percentage = (count / total_tags) * 100
            print(f"{rank:2d}.   {tag:<22} {count:5d}   {percentage:5.1f}%")
    else:
        print("\nâš ï¸ æœªæ‰¾åˆ°ä»»ä½•æ ‡ç­¾")

if __name__ == "__main__":
    # é…ç½®è·¯å¾„
    TARGET_DIR = r"D:\ChatgptKay\kayagi\cpp\luogu\sorted_problems\NOI"  # æŒ‡å®šæ–‡ä»¶å¤¹è·¯å¾„
    OUTPUT_BASE_DIR = r"D:\ChatgptKay\kayagi\cpp\luogu\tag_problems"  # ä¿å­˜ç»“æœçš„æ ¹ç›®å½•
    
    # æŸ¥æ‰¾æ— æ ‡ç­¾é¢˜ç›®
    find_files_without_tags(TARGET_DIR, OUTPUT_BASE_DIR)
    
    # å¯é€‰ï¼šåˆ†ææ ‡ç­¾ç»Ÿè®¡
    analyze_tag_statistics(TARGET_DIR)
    
    # å¦‚æœéœ€è¦æŒ‰ç‰¹å®šæ ‡ç­¾æ•´åˆï¼Œå–æ¶ˆä¸‹é¢çš„æ³¨é‡Šå¹¶æŒ‡å®šæ ‡ç­¾
    # TARGET_TAG = "åŠ¨æ€è§„åˆ’"  # æŒ‡å®šç›®æ ‡æ ‡ç­¾
    # consolidate_files_by_tag(TARGET_DIR, OUTPUT_BASE_DIR, TARGET_TAG)
    
    # åˆå¹¶æ‰€æœ‰MDæ–‡ä»¶
    combine_all_md_files(OUTPUT_BASE_DIR)

 


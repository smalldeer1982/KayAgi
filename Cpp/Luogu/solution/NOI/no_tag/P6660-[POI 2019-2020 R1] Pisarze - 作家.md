# [POI 2019/2020 R1] Pisarze / 作家

## 题目背景

Bajtek 拿到了一些波兰文的句段，他想知道这个这个句段从哪本书来的。

**本题为数据分析题。**

# 请用 C++14/C++17 提交以避免不必要的 CE。

## 题目描述

他找到了 $t$ 个句段，并且确定这些句段来自以下三本书

- Adama Mickiewicza 的 Pan Tadeusz  
- Henryka Sienkiewicza 的 Quo Vadis
- Bolesława Prusa 的 Lalka

求具体来自哪一本书。

## 说明/提示

#### 样例说明

样例只截取了一部分，真正的样例见附加文件中的 sample.in 与 sample.out。

#### 数据规模与约定

**本题采用捆绑测试。**

- Subtask 1（20 pts）：$t \le 100$，$500 \le |s_i| \le 2000$。
- Subtask 2（20 pts）：每行一定为完整的句子。
- Subtask 3（30 pts）：$30 \le |s_i| \le 80$。
- Subtask 4（30 pts）：无特殊限制。

对于 $100\%$ 的数据，$1 \le t \le 1000$，$10 \le |s_i| \le 2000$，$\sum|s_i| \le 2 \times 10^6$。

**本题为数据分析题。**

**本题采用 Special Judge**，您不需要准确识别每一组数据，SPJ 机制如下：

- 假设 $t$ 为句段数，$p$ 为您答对的句段数。
- 如果 $p \ge 0.9 \times t$，那么您就会得到 $100\%$ 的分数。
- 如果 $p \le \dfrac{t}{3}$，很遗憾，您不会得到任何分数。
- 其他情况，您会得到 $100\times \dfrac{p-\frac{t}{3}}{0.9\times t-\frac{t}{3}}\%$ 的分数。

附加文件中的 Mickiewicz.txt，Prus.txt 和 Sienkiewicz.txt 代表三段句段。

数据生成器见附加文件中的 pistestgen.py，使用 `python3 pistestgen.py subtask name directory [seed]` 来生成一组数据，其中：

- grupa 代表子任务的编号，从 $1$ 到 $4$
- nazma 为数据名，将会生成在 nazma.in 与 nazma.out 中
- katalog 为这三本书的文件夹
- ziarno 用于生成同一组数据

#### 说明

翻译自 [POI 2019](https://sio2.mimuw.edu.pl/c/oi27-1/dashboard/) B [Pisarze](https://sio2.mimuw.edu.pl/c/oi27-1/p/pis/)。

## 样例 #1

### 输入

```
3
Petroniusz obudzil sie zaledwie kolo...
Litwo! Ojczyzno moja! ty jestes jak...
W poczatkach roku 1878, kiedy swiat...```

### 输出

```
Sienkiewicz
Mickiewicz
Prus```

# 题解

## 作者：AsunderSquall (赞：37)

update：远古题解，修复了部分意识流表述，并且尽可能地修复了不符合题解格式规范的问题。如果还有格式问题，辛苦管理员指出。

在和 @shenmadongdong 的合作下拿到了 100 分。  
~~不要问我为什么代码和他那么像（其实是一模一样），本来就是合作的~~  
声明：打表程序主要为我写，主程序主要由 shenmadongdong 写。（所以大部分提交都是 shenmadongdong 提交的）  
还有，打表程序已经丢失了（悲）。  

-----------------


之前看到[这个帖子](https://www.luogu.com.cn/discuss/show/270033)说用什么 b2z 压缩，但是我完全不会，所以只能自己瞎摸索。  

首先有一个朴素的想法，把所有单词统计在每个文本中出现的次数，存在一个 `map` 中，再通过某种方法得出单词在文本中出现的频率。  

最后在处理的时候，读入一个串，得到每个单词它在每个文本中出现频率，相乘后，输出最大的值对应的文本。  
这样的准确性比较高，实际上，对于一个文本，只要有一个单词没有在串中出现，它的权值就会直接变为 $0$，因此不会选择这段文本。  

显然这样会超出代码长度限制，因此我们需要压缩。  
首先是最简单的压缩：压入 `map` 的函数很长，我们可以用一个函数来实现。    
`void G(string s,int a,int b,int c){mp[s].p=a;mp[s].s=b;mp[s].m=c;}`
但是单词实在太多了，这样是远远不够的。  

---------------

我的方法可能比较奇怪，就是对于一个单词，每 $3$ 位划分当作一个新的“单词”。  
比如对于单词 $\mathrm{abcdef}$，分解成 $\mathrm{abc\ bcd \ cde\ def}$  $4$ 个单词，然后放入 `map` 中  
这样虽然单词变多了，但是由于只剩下 $3$ 位，有很多重复的了。  
（注：不到 $3$ 位的单词直接存，反正也不多）

------------------

但是这样还是存不下。  
我们考虑将一些用处不是很大的 $3$ 位串去掉。  

假如一个单词在 $3$ 个文本中出现的频率差不多，我们可以不要他。  
`if (Max>Min*X\*X为自己定义的实数*\)`  
我觉得这样操作比较好，因为这样一定可以把**只在部分文本中出现过的单词**存进去。  
经过一定的调参，我们可以把打表程序压到 50kb 以下。  

然后获得 84 分的[好成绩](https://www.luogu.com.cn/record/41002322)。 

--------------------

这样的单词数目还是太少，准确率不够高。  
我们可以再定义 $3$ 个函数，对于函数内部有两个数都是 $0$ 的单词（即只在一个文本中出现的单词）定义一个新的函数，可以略微压缩一些代码长度，因此可以调 `X` 输出更多的单词。  
```cpp
void P(J s){mp[s].p=1,mp[s].s=0,mp[s].m=0;}
void M(J s){mp[s].m=1,mp[s].s=0,mp[s].p=0;}
void S(J s){mp[s].s=1,mp[s].p=0,mp[s].m=0;}
```  
获得 92 分。[link](https://www.luogu.com.cn/record/41007650)  

---------------

P.S.：然后我们走了一些弯路，决定不区分大小写来压缩，但是这样失真过大，最终决定还是区分大小写。  

然后我们觉得用一个函数来搞还是太占字符数，每次都要双引号啊，括号啊什么的挺麻烦的。我们就把单词压入不同的字符串里，进行字符串的处理，这样又能多打一些单词。  
比如说，其中的一段字符串长这样：`spi115,220,50spl47,39,16spo1340,612,196spr446,213,69`。  
可以获得 96 分。[link](https://www.luogu.com.cn/record/41274778)  

--------------------

如果你能看到上面的这一份代码，你会发现字符串 `G3` 中有很多逗号，那是为了区分不同的数字。（注：`Prus.txt` 里面是存在数字的，但是这些数字一定存在 `P1 P2 P3` 字符串中，不会对 `G3` 的判断造成干扰，因为其他两个文本中都没有数字）  

这些逗号占得位置很多，我们考虑怎么把它去掉。  
这里我们把数字用 91 进制表示，用了 美元符号 到 `~` 的所有字符，然后如果单词有超过 `8100` 的数字，就直接用函数，不然的话这个数字一定可以压在 $2$ 位字符中。  

实测压缩效果并不好，因为很多数字都是只有 $1$ 位的。  
因此我们再加了一个特判，如果单词里面的数字没有超过 $91$ 的，那就把它放入另外一个字符串内，这个字符串中每个单词一定都可用 $3$ 个字符来表示参数。  
这样就可以多输出一些单词啦！  
最终我的代码参数 `X` 压到了 1.6（最开始是 15）。  
然后经过主程序中的一些调参，达到了 98 分。  
[link](https://www.luogu.com.cn/record/41363900)  

-------------

经过我们的一些试验，我们发现在上述过程中，`X` 并非是越小越好，大概在 `1.7` 时表现最优。  
那么我们就多出了一些空间，考虑打一些其他的东西。  
我们选择对于每个单词，将它首尾相接拼成一个 $2$ 位字符串，用函数放入另外一个  `map` 中。  
很遗憾，加上这个以后代码又超过 50kb 了  

然后再用类似于上面的处理，把它压入两个字符串中。  
然而还是会超。  

我们再增加一些筛选条件，顺便也把上面的 $3$ 位字符串也筛选一下。  
这里增加了限定如果**总出现次数比较少，就不输出**。  
最后再经过漫长地调参过程（这个过程主要在LOJ上进行），我们达到了 99 分！  
[link](https://www.luogu.com.cn/record/41630786)

-------------------

我们又加入了一些限制条件，在分数下降不严重的情况下省下了另外一些空间。  
然后我们再考虑打一些其他的东西。  

注意到我们还是在对每个单词独立地进行处理，没有考虑到它是一段连续的文本。  
这里我们选择将前一个单词的最后一位和后一个单词的第一位拼起来，再开一个 `map` 存储。  
它又超了。  
如果它的出现总数比较小，那么它在输入中的概率也比较小，那么我们可以不输出它。  
`int SSUM=mp[tmp].p+mp[tmp].s+mp[tmp].m;`  
`if (SSUM>=?)`  
然后我们再进行一些诡异地调参，得到了更好的成绩（只是通过了更多测试点，分数没有提高）  

我们对我们得分最低的点 `#64` 进行了分析

然后，我们发现，如果去掉之前的`P S M`这类函数的操作，对其他点影响不大，但是 `#64` 得分掉得挺多。  
于是我们决定打出一部分 $4$ 位字符串。  
这 $4$ 位字符串，我们选择只截取一个单词的前 $4$ 位，并且这个字符串只在一个文本中出现的时候输出。  
然后限定了一下出现次数，在 LOJ 上获得 PC 100 分。（大概是四舍五入成 100 分了）  
```cpp
if (L==4)
{
    if (PP==0 && (SS==0 && mp[tmp].m>=6)) MA[++cntM[L]][L]=tmp2,cerr<<mp[tmp].m<<endl;
    else if (PP==0 && (MM==0 && mp[tmp].s>=6)) SA[++cntS[L]][L]=tmp2,cerr<<mp[tmp].s<<endl;
    else if (MM==0 && (SS==0 && mp[tmp].p>=6)) PA[++cntP[L]][L]=tmp2,cerr<<mp[tmp].p<<endl;
    continue;
}
```  
（但是在洛谷上还是99分）  

接下来开始乱调参。  
我们把 $3$ 个 $6$ 分别换成 $3,3,9$ 左右（具体记不清了）就能在 LOJ 上 AC 了。  
然而这样还是超出代码限制了（51.5k）。  
然后是一个诡异的操作：我们发现字符串 `S3` 的长度比较长，就把 `S3` 给删了。  
然后依然能 AC，并且代码在长度限制内。  
至于为什么，我也不清楚，反正这本来就是个乱搞题。    
那么我们就愉快地过了这道题啦。  
[link](https://www.luogu.com.cn/record/41678044)   
[AC代码，经过地狱般的压行，谨慎食用](https://www.luogu.com.cn/paste/vu328aq0)  


---

## 作者：一只书虫仔 (赞：26)

注：本题代码和思路均来自 std。

[P6660 POI 2019 B](https://www.luogu.com.cn/problem/P6660)

#### Description

> 给定若干句话，求每句话来自哪个文本。

#### Solution

数据分析题，我猜这题在 POI 2019 中就是用来骗分的，故加上了骗分标签。

因为截止到 2020.8.7 11:45 还没有一个人能写出正解（

这题一种解法就是小林子那种的打表解法，在这里介绍真正的数据分析解法。

首先，第一部分，我们对标点符号和数字进行分析，存一个数组 $Q$，$Q[i][\mathtt{c}]$ 就代表在第 $i$ 个文本字符 $\tt c$ 出现的概率。

剖析完之后得到的结果为

```cpp
Q[0][' ']=0.15634;
Q[0][',']=0.02080;
Q[0]['.']=0.00655;
Q[0][';']=0.00333;
Q[0][':']=0.00267;
Q[0]['"']=0.00243;
Q[0]['!']=0.00239;
Q[0]['-']=0.00144;
Q[0]['?']=0.00099;
Q[0]['*']=0.00032;
Q[0][')']=0.00016;
Q[1][' ']=0.15888;
Q[1]['.']=0.02443;
Q[1][',']=0.01425;
Q[1]['-']=0.00767;
Q[1]['?']=0.00198;
Q[1]['!']=0.00160;
Q[1]['"']=0.00127;
Q[1][':']=0.00095;
Q[1][';']=0.00063;
Q[1][')']=0.00018;
Q[1]['0']=0.00003;
Q[1]['8']=0.00003;
Q[1]['\'']=0.00001;
Q[1]['5']=0.00001;
Q[1]['9']=0.00001;
Q[1]['6']=0.00001;
Q[1]['7']=0.00001;
Q[1]['4']=0.00001;
Q[2][' ']=0.15901;
Q[2][',']=0.01825;
Q[2]['.']=0.00963;
Q[2]['-']=0.00342;
Q[2]['!']=0.00126;
Q[2][':']=0.00107;
Q[2]['?']=0.00088;
Q[2]['"']=0.00075;
Q[2][';']=0.00020;
Q[2][')']=0.00002;
Q[2]['\'']=0.00000;
```

然后是对文本段的单词进行分析，因为是波兰语，所以不需要对每个字母进行概率分析（加上单词分析比字母分析更好）。

取出关键的波兰语词语，进行概率分析，与标点数字分析的原理一样，存一个 $M[i][\mathtt{s}]$ 的数组。

```cpp
M[0]["w"]=0.0220758;
M[0]["sie"]=0.0219952;
M[0]["i"]=0.0207706;
M[0]["z"]=0.0181924;
M[0]["na"]=0.0169194;
M[0]["nie"]=0.0112957;
M[0]["I"]=0.0106189;
M[0]["do"]=0.0077185;
M[0]["ze"]=0.0073640;
M[0]["jak"]=0.0068161;
M[0]["to"]=0.0064133;
M[0]["A"]=0.0049630;
M[0]["po"]=0.0040284;
M[0]["za"]=0.0040284;
M[0]["o"]=0.0038189;
M[0]["juz"]=0.0034806;
M[0]["a"]=0.0032227;
M[0]["tak"]=0.0031744;
M[0]["co"]=0.0030616;
M[0]["od"]=0.0029327;
M[0]["W"]=0.0028844;
M[0]["ja"]=0.0026588;
M[0]["Na"]=0.0024654;
M[0]["Nie"]=0.0022881;
M[0]["byl"]=0.0022720;
M[0]["Z"]=0.0022398;
M[0]["Ze"]=0.0022237;
M[0]["jest"]=0.0021270;
M[0]["nim"]=0.0021109;
M[0]["mu"]=0.0020787;
M[0]["go"]=0.0020626;
M[0]["pan"]=0.0019981;
M[0]["Hrabia"]=0.0018531;
M[0]["Sedzia"]=0.0018370;
M[0]["rzekl"]=0.0018208;
M[0]["Ale"]=0.0017725;
M[0]["gdy"]=0.0017242;
M[0]["Bo"]=0.0017081;
M[0]["Jak"]=0.0017081;
M[0]["mnie"]=0.0016436;
M[0]["tylko"]=0.0016436;
M[0]["ich"]=0.0015953;
M[0]["Lecz"]=0.0015791;
M[0]["jako"]=0.0015469;
M[0]["pod"]=0.0015469;
M[0]["dla"]=0.0015147;
M[0]["tym"]=0.0014825;
M[0]["tam"]=0.0014180;
M[0]["Tadeusz"]=0.0013858;
M[0]["u"]=0.0013697;
M[0]["tu"]=0.0013536;
M[0]["nad"]=0.0013374;
M[0]["Wojski"]=0.0013213;
M[0]["bez"]=0.0013213;
M[0]["lecz"]=0.0013213;
M[0]["ten"]=0.0013213;
M[0]["To"]=0.0012891;
M[0]["jej"]=0.0012891;
M[0]["przez"]=0.0012569;
M[0]["przy"]=0.0012569;
M[0]["przed"]=0.0012408;
M[0]["jeszcze"]=0.0012246;
M[0]["on"]=0.0011924;
M[0]["Tak"]=0.0011763;
M[0]["bylo"]=0.0011763;
M[0]["ku"]=0.0011763;
M[0]["sam"]=0.0011763;
M[0]["Gdy"]=0.0011441;
M[0]["bo"]=0.0011280;
M[0]["ma"]=0.0011280;
M[0]["domu"]=0.0011118;
M[0]["Juz"]=0.0010957;
M[0]["mi"]=0.0010957;
M[0]["Ja"]=0.0010796;
M[0]["jego"]=0.0010796;
M[0]["mial"]=0.0010796;
M[0]["oczy"]=0.0010796;
M[0]["by"]=0.0010635;
M[0]["wszyscy"]=0.0010474;
M[0]["O"]=0.0010313;
M[0]["On"]=0.0010152;
M[0]["nas"]=0.0010152;
M[0]["te"]=0.0010152;
M[0]["czy"]=0.0009990;
M[0]["gdzie"]=0.0009990;
M[0]["Az"]=0.0009346;
M[0]["Gerwazy"]=0.0009185;
M[0]["tez"]=0.0009024;
M[0]["Co"]=0.0008863;
M[0]["Telimena"]=0.0008863;
M[0]["sobie"]=0.0008863;
M[0]["znowu"]=0.0008863;
M[0]["Czy"]=0.0008701;
M[0]["kto"]=0.0008701;
M[0]["az"]=0.0008540;
M[0]["rece"]=0.0008540;
M[0]["choc"]=0.0008379;
M[0]["glowy"]=0.0008379;
M[0]["szlachta"]=0.0008379;
M[0]["tej"]=0.0008379;
M[0]["we"]=0.0008379;
M[1]["sie"]=0.0276917;
M[1]["i"]=0.0266913;
M[1]["nie"]=0.0179453;
M[1]["w"]=0.0175207;
M[1]["na"]=0.0160956;
M[1]["ze"]=0.0153933;
M[1]["z"]=0.0147564;
M[1]["do"]=0.0122371;
M[1]["to"]=0.0089992;
M[1]["a"]=0.0079294;
M[1]["pan"]=0.0069290;
M[1]["Wokulski"]=0.0067453;
M[1]["o"]=0.0057817;
M[1]["za"]=0.0056306;
M[1]["co"]=0.0047242;
M[1]["ja"]=0.0046262;
M[1]["jest"]=0.0046262;
M[1]["jak"]=0.0046221;
M[1]["mu"]=0.0043404;
M[1]["ale"]=0.0043036;
M[1]["mnie"]=0.0041158;
M[1]["go"]=0.0039647;
M[1]["mi"]=0.0038749;
M[1]["pani"]=0.0038422;
M[1]["po"]=0.0037361;
M[1]["tak"]=0.0036585;
M[1]["od"]=0.0035931;
M[1]["juz"]=0.0034951;
M[1]["A"]=0.0032869;
M[1]["tylko"]=0.0032461;
M[1]["nawet"]=0.0032012;
M[1]["jej"]=0.0031807;
M[1]["Nie"]=0.0024335;
M[1]["W"]=0.0023805;
M[1]["Ale"]=0.0023355;
M[1]["tym"]=0.0023070;
M[1]["sobie"]=0.0022988;
M[1]["panna"]=0.0022947;
M[1]["ktory"]=0.0022743;
M[1]["jeszcze"]=0.0022580;
M[1]["jego"]=0.0022008;
M[1]["ma"]=0.0021151;
M[1]["panie"]=0.0020987;
M[1]["czy"]=0.0020620;
M[1]["I"]=0.0020252;
M[1]["dla"]=0.0020130;
M[1]["ten"]=0.0020130;
M[1]["on"]=0.0020089;
M[1]["Izabela"]=0.0019844;
M[1]["moze"]=0.0019721;
M[1]["tej"]=0.0018742;
M[1]["bardzo"]=0.0017802;
M[1]["azeby"]=0.0017435;
M[1]["Wokulskiego"]=0.0017312;
M[1]["byl"]=0.0017149;
M[1]["albo"]=0.0016863;
M[1]["pana"]=0.0016496;
M[1]["nim"]=0.0016169;
M[1]["tego"]=0.0015516;
M[1]["odparl"]=0.0015393;
M[1]["przez"]=0.0015393;
M[1]["u"]=0.0015067;
M[1]["wiec"]=0.0014985;
M[1]["rubli"]=0.0014699;
M[1]["rzekl"]=0.0014168;
M[1]["bo"]=0.0013760;
M[1]["chwili"]=0.0013760;
M[1]["tu"]=0.0013556;
M[1]["bylo"]=0.0013393;
M[1]["kiedy"]=0.0013352;
M[1]["znowu"]=0.0013229;
M[1]["przed"]=0.0013107;
M[1]["czlowiek"]=0.0013066;
M[1]["jezeli"]=0.0012903;
M[1]["ktora"]=0.0012903;
M[1]["mowil"]=0.0012698;
M[1]["dzis"]=0.0012658;
M[1]["tysiecy"]=0.0012576;
M[1]["Pan"]=0.0012535;
M[1]["przy"]=0.0012535;
M[1]["tam"]=0.0012454;
M[1]["niej"]=0.0012372;
M[1]["ich"]=0.0012127;
M[1]["To"]=0.0012086;
M[1]["jednak"]=0.0011841;
M[1]["Rzecki"]=0.0011637;
M[1]["byc"]=0.0011637;
M[1]["nic"]=0.0011637;
M[1]["te"]=0.0011433;
M[1]["Co"]=0.0011310;
M[1]["bedzie"]=0.0011269;
M[1]["niego"]=0.0011229;
M[1]["nas"]=0.0011147;
M[1]["zas"]=0.0011147;
M[1]["wszystko"]=0.0010984;
M[1]["Na"]=0.0010861;
M[1]["siebie"]=0.0010779;
M[1]["sa"]=0.0010412;
M[1]["mial"]=0.0010371;
M[1]["ani"]=0.0010289;
M[1]["pod"]=0.0010208;
M[2]["i"]=0.0381469;
M[2]["sie"]=0.0254748;
M[2]["w"]=0.0215003;
M[2]["nie"]=0.0178763;
M[2]["na"]=0.0160168;
M[2]["ze"]=0.0156782;
M[2]["z"]=0.0147989;
M[2]["do"]=0.0105274;
M[2]["to"]=0.0071173;
M[2]["a"]=0.0064044;
M[2]["o"]=0.0054835;
M[2]["jak"]=0.0052934;
M[2]["ja"]=0.0052518;
M[2]["mu"]=0.0050023;
M[2]["jej"]=0.0045686;
M[2]["jego"]=0.0044676;
M[2]["go"]=0.0044082;
M[2]["po"]=0.0042003;
M[2]["za"]=0.0040933;
M[2]["jest"]=0.0040814;
M[2]["ale"]=0.0039567;
M[2]["Winicjusz"]=0.0036299;
M[2]["od"]=0.0035289;
M[2]["by"]=0.0032794;
M[2]["tak"]=0.0032794;
M[2]["zas"]=0.0032556;
M[2]["co"]=0.0032259;
M[2]["rzekl"]=0.0031962;
M[2]["mnie"]=0.0030952;
M[2]["mi"]=0.0029824;
M[2]["tylko"]=0.0029051;
M[2]["ktory"]=0.0028814;
M[2]["jeszcze"]=0.0027625;
M[2]["ci"]=0.0026437;
M[2]["I"]=0.0026378;
M[2]["ich"]=0.0025843;
M[2]["juz"]=0.0025308;
M[2]["Petroniusz"]=0.0025190;
M[2]["Lecz"]=0.0024893;
M[2]["gdy"]=0.0024893;
M[2]["bylo"]=0.0024596;
M[2]["przez"]=0.0024299;
M[2]["dla"]=0.0023704;
M[2]["tym"]=0.0023645;
M[2]["byl"]=0.0022635;
M[2]["Nie"]=0.0022041;
M[2]["jednak"]=0.0021209;
M[2]["sobie"]=0.0020972;
M[2]["ktora"]=0.0019783;
M[2]["nim"]=0.0019665;
M[2]["A"]=0.0018892;
M[2]["moze"]=0.0018892;
M[2]["poczal"]=0.0018061;
M[2]["chwili"]=0.0017942;
M[2]["tego"]=0.0017942;
M[2]["przy"]=0.0017526;
M[2]["nad"]=0.0017466;
M[2]["wiec"]=0.0017407;
M[2]["jakby"]=0.0017169;
M[2]["nawet"]=0.0016991;
M[2]["Ligia"]=0.0016694;
M[2]["domu"]=0.0016694;
M[2]["lub"]=0.0016575;
M[2]["lecz"]=0.0016516;
M[2]["W"]=0.0015506;
M[2]["ktorych"]=0.0015506;
M[2]["on"]=0.0015328;
M[2]["teraz"]=0.0015328;
M[2]["Winicjusza"]=0.0015209;
M[2]["czym"]=0.0015209;
M[2]["cezara"]=0.0015149;
M[2]["przed"]=0.0015149;
M[2]["cie"]=0.0015031;
M[2]["ktore"]=0.0014912;
M[2]["niego"]=0.0014674;
M[2]["czy"]=0.0014199;
M[2]["Ale"]=0.0013842;
M[2]["Ligii"]=0.0013783;
M[2]["niej"]=0.0013724;
M[2]["sam"]=0.0013605;
M[2]["mogl"]=0.0013545;
M[2]["byc"]=0.0013486;
M[2]["bedzie"]=0.0013367;
M[2]["panie"]=0.0013248;
M[2]["oczy"]=0.0012892;
M[2]["tej"]=0.0012654;
M[2]["Na"]=0.0012535;
M[2]["pod"]=0.0012476;
M[2]["Po"]=0.0012179;
M[2]["cezar"]=0.0012179;
M[2]["albowiem"]=0.0012120;
M[2]["ku"]=0.0011941;
M[2]["mowil"]=0.0011941;
M[2]["jesli"]=0.0011466;
M[2]["ten"]=0.0011466;
M[2]["ty"]=0.0011288;
M[2]["bo"]=0.0011110;
M[2]["nia"]=0.0011110;
M[2]["Nero"]=0.0010931;
M[2]["ludzi"]=0.0010813;
M[2]["te"]=0.0010813;
```

然后最后运用得到的概率进行输入和判断即可，判断的过程就是下面这一段

```cpp
if(dif[0]<=dif[1] && dif[0]<=dif[2])
	printf("Mickiewicz\n");
else if(dif[1]<=dif[0] && dif[1]<=dif[2])
	printf("Prus\n");
else
	printf("Sienkiewicz\n");
```

最后就是整个的代码了！（来自 std）

还放代码是因为给定单词的拆分的分析没放，所以放 std 了。

#### Code

```cpp
#include <bits/stdc++.h>
using namespace std;
#define mpr make_pair
#define FS first
#define SC second
#define PB push_back
double Q[3][555];
map < string , double > M[3];
void makedata(){
	fill(Q[0],Q[0]+555,0.0);
	fill(Q[1],Q[1]+555,0.0);
	fill(Q[2],Q[2]+555,0.0);
	Q[0][' ']=0.15634;
	Q[0][',']=0.02080;
	Q[0]['.']=0.00655;
	Q[0][';']=0.00333;
	Q[0][':']=0.00267;
	Q[0]['"']=0.00243;
	Q[0]['!']=0.00239;
	Q[0]['-']=0.00144;
	Q[0]['?']=0.00099;
	Q[0]['*']=0.00032;
	Q[0][')']=0.00016;
	Q[1][' ']=0.15888;
	Q[1]['.']=0.02443;
	Q[1][',']=0.01425;
	Q[1]['-']=0.00767;
	Q[1]['?']=0.00198;
	Q[1]['!']=0.00160;
	Q[1]['"']=0.00127;
	Q[1][':']=0.00095;
	Q[1][';']=0.00063;
	Q[1][')']=0.00018;
	Q[1]['0']=0.00003;
	Q[1]['8']=0.00003;
	Q[1]['\'']=0.00001;
	Q[1]['5']=0.00001;
	Q[1]['9']=0.00001;
	Q[1]['6']=0.00001;
	Q[1]['7']=0.00001;
	Q[1]['4']=0.00001;
	Q[2][' ']=0.15901;
	Q[2][',']=0.01825;
	Q[2]['.']=0.00963;
	Q[2]['-']=0.00342;
	Q[2]['!']=0.00126;
	Q[2][':']=0.00107;
	Q[2]['?']=0.00088;
	Q[2]['"']=0.00075;
	Q[2][';']=0.00020;
	Q[2][')']=0.00002;
	Q[2]['\'']=0.00000;
M[0]["w"]=0.0220758;
M[0]["sie"]=0.0219952;
M[0]["i"]=0.0207706;
M[0]["z"]=0.0181924;
M[0]["na"]=0.0169194;
M[0]["nie"]=0.0112957;
M[0]["I"]=0.0106189;
M[0]["do"]=0.0077185;
M[0]["ze"]=0.0073640;
M[0]["jak"]=0.0068161;
M[0]["to"]=0.0064133;
M[0]["A"]=0.0049630;
M[0]["po"]=0.0040284;
M[0]["za"]=0.0040284;
M[0]["o"]=0.0038189;
M[0]["juz"]=0.0034806;
M[0]["a"]=0.0032227;
M[0]["tak"]=0.0031744;
M[0]["co"]=0.0030616;
M[0]["od"]=0.0029327;
M[0]["W"]=0.0028844;
M[0]["ja"]=0.0026588;
M[0]["Na"]=0.0024654;
M[0]["Nie"]=0.0022881;
M[0]["byl"]=0.0022720;
M[0]["Z"]=0.0022398;
M[0]["Ze"]=0.0022237;
M[0]["jest"]=0.0021270;
M[0]["nim"]=0.0021109;
M[0]["mu"]=0.0020787;
M[0]["go"]=0.0020626;
M[0]["pan"]=0.0019981;
M[0]["Hrabia"]=0.0018531;
M[0]["Sedzia"]=0.0018370;
M[0]["rzekl"]=0.0018208;
M[0]["Ale"]=0.0017725;
M[0]["gdy"]=0.0017242;
M[0]["Bo"]=0.0017081;
M[0]["Jak"]=0.0017081;
M[0]["mnie"]=0.0016436;
M[0]["tylko"]=0.0016436;
M[0]["ich"]=0.0015953;
M[0]["Lecz"]=0.0015791;
M[0]["jako"]=0.0015469;
M[0]["pod"]=0.0015469;
M[0]["dla"]=0.0015147;
M[0]["tym"]=0.0014825;
M[0]["tam"]=0.0014180;
M[0]["Tadeusz"]=0.0013858;
M[0]["u"]=0.0013697;
M[0]["tu"]=0.0013536;
M[0]["nad"]=0.0013374;
M[0]["Wojski"]=0.0013213;
M[0]["bez"]=0.0013213;
M[0]["lecz"]=0.0013213;
M[0]["ten"]=0.0013213;
M[0]["To"]=0.0012891;
M[0]["jej"]=0.0012891;
M[0]["przez"]=0.0012569;
M[0]["przy"]=0.0012569;
M[0]["przed"]=0.0012408;
M[0]["jeszcze"]=0.0012246;
M[0]["on"]=0.0011924;
M[0]["Tak"]=0.0011763;
M[0]["bylo"]=0.0011763;
M[0]["ku"]=0.0011763;
M[0]["sam"]=0.0011763;
M[0]["Gdy"]=0.0011441;
M[0]["bo"]=0.0011280;
M[0]["ma"]=0.0011280;
M[0]["domu"]=0.0011118;
M[0]["Juz"]=0.0010957;
M[0]["mi"]=0.0010957;
M[0]["Ja"]=0.0010796;
M[0]["jego"]=0.0010796;
M[0]["mial"]=0.0010796;
M[0]["oczy"]=0.0010796;
M[0]["by"]=0.0010635;
M[0]["wszyscy"]=0.0010474;
M[0]["O"]=0.0010313;
M[0]["On"]=0.0010152;
M[0]["nas"]=0.0010152;
M[0]["te"]=0.0010152;
M[0]["czy"]=0.0009990;
M[0]["gdzie"]=0.0009990;
M[0]["Az"]=0.0009346;
M[0]["Gerwazy"]=0.0009185;
M[0]["tez"]=0.0009024;
M[0]["Co"]=0.0008863;
M[0]["Telimena"]=0.0008863;
M[0]["sobie"]=0.0008863;
M[0]["znowu"]=0.0008863;
M[0]["Czy"]=0.0008701;
M[0]["kto"]=0.0008701;
M[0]["az"]=0.0008540;
M[0]["rece"]=0.0008540;
M[0]["choc"]=0.0008379;
M[0]["glowy"]=0.0008379;
M[0]["szlachta"]=0.0008379;
M[0]["tej"]=0.0008379;
M[0]["we"]=0.0008379;
M[1]["sie"]=0.0276917;
M[1]["i"]=0.0266913;
M[1]["nie"]=0.0179453;
M[1]["w"]=0.0175207;
M[1]["na"]=0.0160956;
M[1]["ze"]=0.0153933;
M[1]["z"]=0.0147564;
M[1]["do"]=0.0122371;
M[1]["to"]=0.0089992;
M[1]["a"]=0.0079294;
M[1]["pan"]=0.0069290;
M[1]["Wokulski"]=0.0067453;
M[1]["o"]=0.0057817;
M[1]["za"]=0.0056306;
M[1]["co"]=0.0047242;
M[1]["ja"]=0.0046262;
M[1]["jest"]=0.0046262;
M[1]["jak"]=0.0046221;
M[1]["mu"]=0.0043404;
M[1]["ale"]=0.0043036;
M[1]["mnie"]=0.0041158;
M[1]["go"]=0.0039647;
M[1]["mi"]=0.0038749;
M[1]["pani"]=0.0038422;
M[1]["po"]=0.0037361;
M[1]["tak"]=0.0036585;
M[1]["od"]=0.0035931;
M[1]["juz"]=0.0034951;
M[1]["A"]=0.0032869;
M[1]["tylko"]=0.0032461;
M[1]["nawet"]=0.0032012;
M[1]["jej"]=0.0031807;
M[1]["Nie"]=0.0024335;
M[1]["W"]=0.0023805;
M[1]["Ale"]=0.0023355;
M[1]["tym"]=0.0023070;
M[1]["sobie"]=0.0022988;
M[1]["panna"]=0.0022947;
M[1]["ktory"]=0.0022743;
M[1]["jeszcze"]=0.0022580;
M[1]["jego"]=0.0022008;
M[1]["ma"]=0.0021151;
M[1]["panie"]=0.0020987;
M[1]["czy"]=0.0020620;
M[1]["I"]=0.0020252;
M[1]["dla"]=0.0020130;
M[1]["ten"]=0.0020130;
M[1]["on"]=0.0020089;
M[1]["Izabela"]=0.0019844;
M[1]["moze"]=0.0019721;
M[1]["tej"]=0.0018742;
M[1]["bardzo"]=0.0017802;
M[1]["azeby"]=0.0017435;
M[1]["Wokulskiego"]=0.0017312;
M[1]["byl"]=0.0017149;
M[1]["albo"]=0.0016863;
M[1]["pana"]=0.0016496;
M[1]["nim"]=0.0016169;
M[1]["tego"]=0.0015516;
M[1]["odparl"]=0.0015393;
M[1]["przez"]=0.0015393;
M[1]["u"]=0.0015067;
M[1]["wiec"]=0.0014985;
M[1]["rubli"]=0.0014699;
M[1]["rzekl"]=0.0014168;
M[1]["bo"]=0.0013760;
M[1]["chwili"]=0.0013760;
M[1]["tu"]=0.0013556;
M[1]["bylo"]=0.0013393;
M[1]["kiedy"]=0.0013352;
M[1]["znowu"]=0.0013229;
M[1]["przed"]=0.0013107;
M[1]["czlowiek"]=0.0013066;
M[1]["jezeli"]=0.0012903;
M[1]["ktora"]=0.0012903;
M[1]["mowil"]=0.0012698;
M[1]["dzis"]=0.0012658;
M[1]["tysiecy"]=0.0012576;
M[1]["Pan"]=0.0012535;
M[1]["przy"]=0.0012535;
M[1]["tam"]=0.0012454;
M[1]["niej"]=0.0012372;
M[1]["ich"]=0.0012127;
M[1]["To"]=0.0012086;
M[1]["jednak"]=0.0011841;
M[1]["Rzecki"]=0.0011637;
M[1]["byc"]=0.0011637;
M[1]["nic"]=0.0011637;
M[1]["te"]=0.0011433;
M[1]["Co"]=0.0011310;
M[1]["bedzie"]=0.0011269;
M[1]["niego"]=0.0011229;
M[1]["nas"]=0.0011147;
M[1]["zas"]=0.0011147;
M[1]["wszystko"]=0.0010984;
M[1]["Na"]=0.0010861;
M[1]["siebie"]=0.0010779;
M[1]["sa"]=0.0010412;
M[1]["mial"]=0.0010371;
M[1]["ani"]=0.0010289;
M[1]["pod"]=0.0010208;
M[2]["i"]=0.0381469;
M[2]["sie"]=0.0254748;
M[2]["w"]=0.0215003;
M[2]["nie"]=0.0178763;
M[2]["na"]=0.0160168;
M[2]["ze"]=0.0156782;
M[2]["z"]=0.0147989;
M[2]["do"]=0.0105274;
M[2]["to"]=0.0071173;
M[2]["a"]=0.0064044;
M[2]["o"]=0.0054835;
M[2]["jak"]=0.0052934;
M[2]["ja"]=0.0052518;
M[2]["mu"]=0.0050023;
M[2]["jej"]=0.0045686;
M[2]["jego"]=0.0044676;
M[2]["go"]=0.0044082;
M[2]["po"]=0.0042003;
M[2]["za"]=0.0040933;
M[2]["jest"]=0.0040814;
M[2]["ale"]=0.0039567;
M[2]["Winicjusz"]=0.0036299;
M[2]["od"]=0.0035289;
M[2]["by"]=0.0032794;
M[2]["tak"]=0.0032794;
M[2]["zas"]=0.0032556;
M[2]["co"]=0.0032259;
M[2]["rzekl"]=0.0031962;
M[2]["mnie"]=0.0030952;
M[2]["mi"]=0.0029824;
M[2]["tylko"]=0.0029051;
M[2]["ktory"]=0.0028814;
M[2]["jeszcze"]=0.0027625;
M[2]["ci"]=0.0026437;
M[2]["I"]=0.0026378;
M[2]["ich"]=0.0025843;
M[2]["juz"]=0.0025308;
M[2]["Petroniusz"]=0.0025190;
M[2]["Lecz"]=0.0024893;
M[2]["gdy"]=0.0024893;
M[2]["bylo"]=0.0024596;
M[2]["przez"]=0.0024299;
M[2]["dla"]=0.0023704;
M[2]["tym"]=0.0023645;
M[2]["byl"]=0.0022635;
M[2]["Nie"]=0.0022041;
M[2]["jednak"]=0.0021209;
M[2]["sobie"]=0.0020972;
M[2]["ktora"]=0.0019783;
M[2]["nim"]=0.0019665;
M[2]["A"]=0.0018892;
M[2]["moze"]=0.0018892;
M[2]["poczal"]=0.0018061;
M[2]["chwili"]=0.0017942;
M[2]["tego"]=0.0017942;
M[2]["przy"]=0.0017526;
M[2]["nad"]=0.0017466;
M[2]["wiec"]=0.0017407;
M[2]["jakby"]=0.0017169;
M[2]["nawet"]=0.0016991;
M[2]["Ligia"]=0.0016694;
M[2]["domu"]=0.0016694;
M[2]["lub"]=0.0016575;
M[2]["lecz"]=0.0016516;
M[2]["W"]=0.0015506;
M[2]["ktorych"]=0.0015506;
M[2]["on"]=0.0015328;
M[2]["teraz"]=0.0015328;
M[2]["Winicjusza"]=0.0015209;
M[2]["czym"]=0.0015209;
M[2]["cezara"]=0.0015149;
M[2]["przed"]=0.0015149;
M[2]["cie"]=0.0015031;
M[2]["ktore"]=0.0014912;
M[2]["niego"]=0.0014674;
M[2]["czy"]=0.0014199;
M[2]["Ale"]=0.0013842;
M[2]["Ligii"]=0.0013783;
M[2]["niej"]=0.0013724;
M[2]["sam"]=0.0013605;
M[2]["mogl"]=0.0013545;
M[2]["byc"]=0.0013486;
M[2]["bedzie"]=0.0013367;
M[2]["panie"]=0.0013248;
M[2]["oczy"]=0.0012892;
M[2]["tej"]=0.0012654;
M[2]["Na"]=0.0012535;
M[2]["pod"]=0.0012476;
M[2]["Po"]=0.0012179;
M[2]["cezar"]=0.0012179;
M[2]["albowiem"]=0.0012120;
M[2]["ku"]=0.0011941;
M[2]["mowil"]=0.0011941;
M[2]["jesli"]=0.0011466;
M[2]["ten"]=0.0011466;
M[2]["ty"]=0.0011288;
M[2]["bo"]=0.0011110;
M[2]["nia"]=0.0011110;
M[2]["Nero"]=0.0010931;
M[2]["ludzi"]=0.0010813;
M[2]["te"]=0.0010813;
}
char s[2000005];
double cur[555];
map < string , double > curmp;
int tot;
vector < string > getstr(char *s,int len){
	int i,j,k;
	vector < string > ret;tot=0;
	for(i=0;i<len;){
		for(j=i;j<len;++j){
			if(!(s[j]>='a' && s[j]<='z') && !(s[j]>='A' && s[j]<='Z'))
				break;
		}
		if(j>i+1){
			ret.emplace_back(s+i,s+j);
//			cout<<ret.back()<<endl;
			++tot;
		}
		i=j+1;
	}
	return ret;
}
int main(){
	int i,j,k,t;
	makedata();
	scanf("%d",&t);getchar();
	while(t--){
		cin.getline(s,(int)2e6);
		int len=strlen(s);
		fill(cur,cur+555,0.0);
		curmp.clear();
		int fc2=0;
		for(i=0;i<len;++i){
			if(s[i]>='a' && s[i]<='z') continue;
			if(s[i]>='A' && s[i]<='Z') continue;
			if(s[i]=='\n') continue;
			cur[s[i]]+=1/(double)len;
			++fc2;
		}
		vector < string > str=getstr(s,len);
		for(i=0;i<(int)str.size();++i){
			curmp[str[i]]+=1/(double)tot;
		}
		double dif[3]={0,0,0};
		double ratio=(double)fc2*(tot>15?3.0:0.1)/(double)tot;
		for(i=0;i<3;++i){
			for(j=0;j<555;++j){
				if(j==(int)' ') continue;
				dif[i]+=fabs(Q[i][j]-cur[j])*ratio;
			}
		}
		map < string , double >::iterator it;
		for(it=curmp.begin();it!=curmp.end();++it){
			for(i=0;i<3;++i){
				dif[i]+=fabs(M[i][it->FS]-it->SC);
			}
		}
		if(dif[0]<=dif[1] && dif[0]<=dif[2])
			printf("Mickiewicz\n");
		else if(dif[1]<=dif[0] && dif[1]<=dif[2])
			printf("Prus\n");
		else
			printf("Sienkiewicz\n");
	}
	return 0;
}
// If encounter problem while debugging, try switching compiler to TDM-GCC-64.
```

然后我们得到了 [3397 pts](https://www.luogu.com.cn/record/35533152) 的好成绩！

其实不是，3397 pts 是 spj 坏掉之前的分数（

这个代码可以获得 [52 pts](https://www.luogu.com.cn/record/35534395) 的成绩，因为这个代码还是有运气的成分和概率的成分。

谁知道打表比正解的数据分析得的分还要高 /lb

By Shuchong       
2020.8.7

---

## 作者：⚡小林子⚡ (赞：21)

## 吐槽

~~感觉这题的神仙程度不亚于 AT678 啊~~

~~文本分析 NOI 系列不考啊~~

~~POI 太毒瘤了~~

## 正文

### 说在前面

由于我不是专门研究文本分析这一块东西的，所以可能思路和代码都很 naive ，欢迎有其他思路的神仙写题解。

Tip：如果以后更改算法导致了分数变高的话，我会第一时间更新此题解。

------------

[题目传送门](https://www.luogu.com.cn/problem/P6660)

首先题意都能看懂吧：给你三大段文本，输入中有 n 条语句，问这些语句都是哪个文本里的。

然后就可以开始想想怎么处理这三大段文本了。

最容易想到的肯定是三段分别去重+统计单词出现次数了。

这一部分 Code 其实是挺好打的（只要你能想到思路）：

```cpp
#include<iostream>
#include<map> 
#include<fstream>
using namespace std;
string s,name,nametxt;
map<string,int>word;
int main(){
	cin>>name;
	nametxt=name+".txt";
	name=name+".out";
	freopen(nametxt.c_str(),"r",stdin);
	freopen(name.c_str(),"w",stdout);
	while(cin>>s)
		word[s]++;
	for(map<string,int>::iterator it=word.begin();it!=word.end();it++)
		cout<<it->first<<" "<<it->second<<endl;
	return 0;
}
```

注：这个代码需要你输入作者名（当初做的时候就是这么做的，正好还能防止一次性抄袭）。

------------

第一步做完，你就有了三个处理过的 .out 文件了。

接下来呢？

我这题的思路十分 naive：找到每个文章专属的词语（在其他文章中不存在的），打表然后回答询问。

接下来我是先将三组数据合并起来的，这样方便找专属词语。

Code：

```cpp
#include<map>
#include<iostream>
#include<fstream>
using namespace std;
string s;int t;
map<string,pair<string,int> >m;  //pair<string,int>:string->Author name,int-appear >num
int main(){
	ifstream Prus("Prus.out");
	while(Prus>>s>>t)
		m[s].first.append("Prus"),m[s].second+=t; 
	ifstream Mickiewicz("Mickiewicz.out");
	while(Mickiewicz>>s>>t)
		m[s].first.append("Mickiewicz"),m[s].second+=t; 
	ifstream Sienkiewicz("Sienkiewicz.out");
	while(Sienkiewicz>>s>>t)
		m[s].first.append("Sienkiewicz"),m[s].second+=t; 
	freopen("data.txt","w",stdout);
	for(map<string,pair<string,int> >::iterator it=m.begin();it!=m.end();it++)
		cout<<it->first<<" "<<(it->second).first<<" "<<(it->second).second<<endl;
	return 0;
}
```

------------

所以下一步就是这个找专属词语了。

专属词语有个条件：因为不可能储存所有的词语(长度炸了），故按照出现次数从大到小排（这样能提高文本中匹配到的几率）。

找专属词语的代码：

```cpp
#include<iostream>
#include<fstream>
#include<algorithm>
#include<string>
using namespace std;
string s,name;int n,t;
struct Node{
	string word,name;
	int num;
	bool operator < (const Node &rhs) const {
		return num>rhs.num;
	}
}a[100001];
int main(){
	freopen("data.txt","r",stdin);
	freopen("need.txt","w",stdout);
	while(cin>>s>>name>>t)
		if(name.size()<=11)
			a[++n]=(Node){s,name,t};
	sort(a+1,a+1+n);
	for(int i=1;i<=n;i++)
		cout<<a[i].word<<" "<<a[i].name<<" "<<a[i].num<<endl;
	return 0;
}
```

解释一下为什么是 11：因为三个作者名中最长的名字是 11，故用 11。

------------

说句实话，这题就是在跟你谷提交长度限制做斗争(你谷最多 50KB）。

我们希望能打表的数量尽量的多，所以下一步要过滤专属词语（将出现次数少的都过滤掉）。

我是按照上一步数据一点一点看哪里行的，发现当出现次数>5 时数据<50KB，>4 时数据>50KB，所以我的代码就写 num>4 了（~~如此宝贵的空间怎么能浪费呢~~）

照着这个界限，写代码就行了。

Code:

```cpp
#include<string>
#include<iostream>
#include<fstream>
using namespace std;
string word,name;int num;
int main(){
	freopen("need.txt","r",stdin);
	freopen("ok.txt","w",stdout);
	while(cin>>word>>name>>num)
		if(num>4)
			cout<<word<<" "<<name<<" "<<num<<endl;
	return 0;
}
```

------------

然后嘞？

然后你需要把数据按照 50KB=51200B 来过滤一下，由于之前按从大到小排过序，所以只需删掉最后的就行。

除去打表剩下的代码大约 200B，也就是说我们能打 51000B 的表。

`const int MAX=51000;`

这一步只需要将上阶段数据给重新读入处理就行了。

```cpp
#include<string>
#include<iostream>
using namespace std;
const int MAX=51000;
string word,name;int num,now;
int main(){
	freopen("ok.txt","r",stdin);
	freopen("somecode.txt","w",stdout);
	while(cin>>word>>name>>num)
		if(now+9+word.size()+name.size()<=MAX)
			cout<<"m[\""<<word<<"\"]=\""<<name<<"\";",now+=9+word.size()+name.size();
		else break;
	return 0;
}
```

emmm漏说一句，这里我将控制长度+制造代码放到了一块。

为什么不换行：换行占长度啊，长度肯定得省着用对不对（

------------

最后就是最激动人心的时刻了——写 代 码 ！

打出一个框架：

```cpp
#include<map>
#include<iostream>
#include<sstream>
#include<cstdio>
using namespace std;
map<string,string>m;
int n;string line,s;
void init(){
  
}
int main(){
	init();
	cin>>n;getchar();
	for(int i=1;i<=n;i++){
		getline(cin,line);
		stringstream sin(line);
		bool flag=0;
		while(sin>>s)
			if(m[s]!=""){
				cout<<m[s]<<endl;
				flag=1;
				break;
			}
		if(!flag) puts("Mickiewicz"); //☆
	}
	return 0;
}
```

把上一步的东西复制到 init 里即可。

但 是 ！

我们在处理第一步的时候没有过滤标点，所以可能有双引号，要你手动加反斜杠（这个其实可以对着编译器报错的位置一个一个改）。

为什么不过滤标点：这样处理匹配精确几率更大。

打星号的地方讲一下：

这个 M 开头的人写的东西最少，也就是说专属单词理论上来说就会最少，所以匹配不到输出他能使得到的分能最大化。

如果代码是 50KB 出头，那就将最后再删掉一点就行了。

------------

结 束 啦 ！

这题其实是昨晚打完模拟后想再来一题，然后想起来这题就开始打的。

本来找 lmpp 结果他还不打（

思路挺难想的，我中途思路也换了 3、4 次这个样子。

最后，挂上 78 分评测记录（这题一共就 100 分）：[Link](https://www.luogu.com.cn/record/36383776)

# $\sf{The\,\,End.}$

---

## 作者：HYdroKomide (赞：7)

### 前言：
1. 菜鸡 HydroKomide 在 CSP-S 与 WC 双双打铁，于是一怒之下开始爆肝非传统题目。

2. 目前，这是一篇非 AC 题解，目前分数为 [93pts](https://www.luogu.com.cn/record/103561082)（~~毕竟这种玄学题目需要经过大量调参才能 A~~）。

3. 感谢机房大佬 [xiaozengX ](https://www.luogu.com.cn/user/321529)、[王君诺](https://www.luogu.com.cn/user/539529)、[青莲武者](https://www.luogu.com.cn/user/325833) 和 [蒟蒻Ivan ](https://www.luogu.com.cn/user/531036) 等人的思路与精神上的支持。

4. 本篇题解未完待续。

### 思路：
第一篇题解的想法很神，切成长度为 $3$ 的小块在能够保证正确率的情况下使空间消耗尽量小。

但是存储数字的那一步略显多余。我们考虑：怎样在不存具体数字的情况下最大化正确率？

此处省略机房大辩论 $114514$ 字。

我们发现，扫描到一个三段字符时，只有可能发生以下两种情况：

1. 使某一篇文章权值提高。
2. 使某一或两篇文章不可能为答案（即完全排除可能性）。

而这两种信息可以直接用两个字符维护。

举例：一段信息为 `abc24`。

其中，`2` 代表给 $2$ 号文章加权值。`4` 二进制为 `100`，即完全排除 $1$ 号文章。

这样，当我们扫描到 `abc` 这个子串，直接加权值、排除文章即可。

### 说这么多，具体实现呢？
文本分析使用了多个程序逐步推进，相对比较复杂但思路顺畅。

首先是拆解原文。把文章中每类的三连字符都计算个数。当然，由于文章长度不同，字符的相对权值肯定要乘上一个系数。

这里使用了 `map`：

```cpp
#include<cstdio>
#include<fstream>
#include<map>
#include<iostream>
using namespace std;
map<string,int>mp;
char a,b,c;
int main(){
	freopen("文章名.txt","r",stdin);
	freopen("ANALYZE_文章名.txt","w",stdout);
	string str="   ";
	scanf("%c%c%c",&a,&b,&c);
	do{
		str[0]=a,str[1]=b,str[2]=c;
		mp[str]++;//每扫描一个字符存一次
		a=b,b=c;
	}while(scanf("%c",&c)!=EOF);
	for(char i='!';i<='z';i++)
		for(char j='!';j<='z';j++)
			for(char k='!';k<='z';k++){//枚举所有可能性，这里省略了空格
				string sss="   ";
				sss[0]=i,sss[1]=j;sss[2]=k;
				if(mp[sss]!=0)cout<<sss<<" "<<mp[sss]*文章系数<<"\n";
			}
	return 0;
}
```

每篇文章都用这个程序跑一遍，得到三个包含字符 + 相对权值的文本文档。

然后就是生成数据库了。把三个分析结果的文本文档都读一遍，然后用上文所提的思路存储下来。

为了更高的准确率，我们需要设定一个阈值系数，即容错率。确保一篇文章的相对权值显著高于另两篇文章。这里设为 $1.6$。

```cpp
#include<fstream>
#include<map>
#include<cstring>
#include<iostream>
#define ri register int
using namespace std;
const double COR=1.6;//阈值系数
map<string,int>Smp,Mmp,Pmp;
string str;
int cnt;
int main(){
	freopen("PROGRAM.txt","w",stdout);
	ifstream Sin("ANALYZE_S.txt");
	ifstream Min("ANALYZE_M.txt");
	ifstream Pin("ANALYZE_P.txt");//上个程序生成的文档
	while(Sin>>str){
		Sin>>cnt;
		Smp[str]=cnt;
	}
	while(Min>>str){
		Min>>cnt;
		Mmp[str]=cnt;
	}
	while(Pin>>str){
		Pin>>cnt;
		Pmp[str]=cnt;//全部读入map
	}
	for(char i='!';i<='z';i++)
		for(char j='!';j<='z';j++)
			for(char k='!';k<='z';k++){
				string sss="   ";
				sss[0]=i,sss[1]=j,sss[2]=k;
				int del=0,pls=0;
				double S=0,M=0,P=0;
				if(Smp.count(sss)!=0)S=Smp[sss];
				if(Mmp.count(sss)!=0)M=Mmp[sss];
				if(Pmp.count(sss)!=0)P=Pmp[sss];
				if(S==0&&M==0&&P==0)continue;
				if(S==0)del|=(1<<2);
				if(M==0)del|=(1<<1);
				if(P==0)del|=(1<<0);//如果文章权值为0，即可完全排除这篇文章
				if(S>M*COR&&S>P*COR)pls=1;
				if(M>S*COR&&M>P*COR)pls=2;
				if(P>S*COR&&P>M*COR)pls=3;//确保阈值
				if(pls==0&&del==0)continue;
				printf("%c%c%c%d%d",i,j,k,pls,del);
			}
	return 0;
}
```

当然，由于我们带上了标点符号，所以还要在生成的所有双引号前加上 `\`。最后结果是[这样](https://www.luogu.com.cn/paste/pi4njkzb)一个东西。

所以能够提交的程序也就来了：[link](https://www.luogu.com.cn/paste/sgf99agn)

### 后记：

玄学题目是最好骗分、最具思维性，但最不容易 AC 的一类题目，同时也最贴近日常应用的。~~所以为什么 OI 反而不考这一类呢？~~

当然，还没用高进制压缩就有 35KB 的长度，这也给后续的工作留下了空间。经对拍，在 $s$ 长度为 $100$ 左右时本代码正确率显著劣于题解的 AC 代码。下一步可以考虑存一些长段的语句辅助判断，或者对于长句子使用哈希。

有错误欢迎评论指正，其他好思路欢迎与 HydroKomide 私信讨论（

### THE END

---


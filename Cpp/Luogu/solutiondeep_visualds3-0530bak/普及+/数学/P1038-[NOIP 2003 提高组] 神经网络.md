# 题目信息

# [NOIP 2003 提高组] 神经网络

## 题目背景

人工神经网络（Artificial Neural Network）是一种新兴的具有自我学习能力的计算系统，在模式识别、函数逼近及贷款风险评估等诸多领域有广泛的应用。对神经网络的研究一直是当今的热门方向，兰兰同学在自学了一本神经网络的入门书籍后，提出了一个简化模型，他希望你能帮助他用程序检验这个神经网络模型的实用性。



## 题目描述

在兰兰的模型中，神经网络就是一张有向图，图中的节点称为神经元，而且两个神经元之间至多有一条边相连，下图是一个神经元的例子：

![](https://cdn.luogu.com.cn/upload/image_hosting/61qm40kj.png)

神经元（编号为 $i$）


图中，$X_1 \sim X_3$ 是信息输入渠道，$Y_1 \sim Y_2$ 是信息输出渠道，$C_i$ 表示神经元目前的状态，$U_i$ 是阈值，可视为神经元的一个内在参数。

神经元按一定的顺序排列，构成整个神经网络。在兰兰的模型之中，神经网络中的神经元分为几层；称为输入层、输出层，和若干个中间层。每层神经元只向下一层的神经元输出信息，只从上一层神经元接受信息。下图是一个简单的三层神经网络的例子。

![](https://cdn.luogu.com.cn/upload/image_hosting/4xd7f8yz.png)

兰兰规定，$C_i$ 服从公式：（其中 $n$ 是网络中所有神经元的数目）

$$C_i=\left(\sum\limits_{(j,i) \in E} W_{ji}C_{j}\right)-U_{i}$$

公式中的 $W_{ji}$（可能为负值）表示连接 $j$ 号神经元和 $i$ 号神经元的边的权值。当 $C_i$ 大于 $0$ 时，该神经元处于兴奋状态，否则就处于平静状态。当神经元处于兴奋状态时，下一秒它会向其他神经元传送信号，信号的强度为 $C_i$。

如此．在输入层神经元被激发之后，整个网络系统就在信息传输的推动下进行运作。现在，给定一个神经网络，及当前输入层神经元的状态（$C_i$），要求你的程序运算出最后网络输出层的状态。



## 说明/提示

**【题目来源】**

NOIP 2003 提高组第一题

## 样例 #1

### 输入

```
5 6
1 0
1 0
0 1
0 1
0 1
1 3 1
1 4 1
1 5 1
2 3 1
2 4 1
2 5 1
```

### 输出

```
3 1
4 1
5 1
```

# AI分析结果



**唯一算法分类**：无算法分类（拓扑排序）

---

### **题解思路、算法要点与解决难点**

#### **核心数学逻辑**
题目要求模拟神经网络信号传递过程，本质是DAG上的层序处理：
1. **公式**：$C_i = \sum (W_{ji} \times C_j) - U_i$  
   - 输入层直接使用初始$C_i$，其他层在计算时需减去阈值$U_i$
   - 仅当$C_i>0$时传递信号到下一层
2. **拓扑排序**：确保按层处理，避免循环依赖
3. **输入层预处理**：初始状态非零的神经元直接入队，其他层在计算时减$U_i$

#### **关键难点与解决**
1. **输入层阈值处理**：输入层的$U_i$无需扣除（题解中通过`if(c[i])`判断）
2. **信号传递条件**：仅在$C_i>0$时向后传递（避免无效计算）
3. **输出层判定**：出度为0且$C_i>0$的点为有效输出

---

### **题解评分（≥4星）**
1. **Lucaster_ (5星)**  
   - **亮点**：预处理阈值，邻接表+队列拓扑，代码可读性高  
   - **关键代码**：
     ```cpp
     if(c[i]) { q.push(i); vis[i]=true; }
     else c[i] -= x; // 非输入层直接减U_i
     ```
2. **zzlzk (4星)**  
   - **亮点**：反向建图+记忆化搜索，避免显式拓扑排序  
   - **心得**："输入层状态不受阈值影响"的预处理判断
3. **teafrogsf (4星)**  
   - **亮点**：显式分层处理，通过`dep[]`数组记录层数  
   - **优化**：层序计算后统一减阈值，减少条件判断

---

### **最优思路提炼**
1. **拓扑排序处理DAG**：用队列管理激活的神经元，确保层序正确
2. **阈值预处理**：非输入层初始化时直接减$U_i$，简化后续计算
3. **信号传递剪枝**：仅当$C_i>0$时触发传递，减少无效计算
4. **输出层标记**：记录出度为0的点，最终筛选$C_i>0$的结果

---

### **同类型题与通用套路**
1. **DAG上的层序处理**：如任务调度、依赖解析（如Makefile）
2. **信号传递模型**：如P4017（最大食物链计数）、P1113（杂务）
3. **通用解法套路**：
   - 拓扑排序确保处理顺序
   - 预处理初始状态（如输入层）
   - 动态更新下游节点状态

---

### **推荐相似题目**
1. **P1113 杂务** - DAG最长路径，拓扑排序应用
2. **P4017 最大食物链计数** - DAG路径统计，层序处理
3. **P1983 车站分级** - 拓扑排序处理隐含层级关系

---

### **可视化与算法演示**
#### **动画设计**
1. **像素风格神经元**：  
   - 每个神经元用8-bit方块表示，颜色区分状态（绿色：激活，红色：平静）
   - 当前处理节点高亮为黄色，传递路径用闪烁线条表示
2. **队列可视化**：右侧显示处理队列，节点按入队顺序排列
3. **公式分步计算**：  
   ```python
   # 伪代码演示步骤
   for 当前节点 in 队列:
       高亮当前节点
       显示当前C_i值
       if C_i > 0:
           for 每个邻居节点:
               显示传递的W_ij * C_i
               更新邻居的C_j并显示
               播放"传递音效"
       else:
           显示"不传递"提示
   ```
4. **音效与交互**：  
   - 关键步骤触发8-bit音效（如激活音、传递音）
   - 支持暂停/步进，观察中间状态变化

#### **复古游戏化实现**
- **Canvas绘制**：使用`<canvas>`绘制神经元网格和传递动画
- **自动演示模式**：AI按拓扑序自动触发计算，类似“自动贪吃蛇”
- **积分系统**：正确传递一次+10分，错误操作扣分

---

### **代码核心实现**
```cpp
// Lucaster_ 题解关键片段
void build(int u, int v, int w) {
    e[++cnt] = {v, w, hd[u]};
    hd[u] = cnt;
    out[u] = true; // 标记有出边
}

while (!q.empty()) {
    int h = q.front(); q.pop();
    if (c[h] <= 0) continue;
    for (int i = hd[h]; i; i = e[i].next) {
        int t = e[i].to;
        c[t] += e[i].val * c[h];
        if (!vis[t]) {
            q.push(t);
            vis[t] = true;
        }
    }
}
// 输出层判断
for (int i = 1; i <= n; i++)
    if (!out[i] && c[i] > 0) 
        printf("%d %d\n", i, c[i]);
```

---

### **个人心得摘录**
- **Lucaster_调试经历**："初始未处理输入层阈值，WA后通过打印中间状态发现逻辑错误"
- **zzlzk优化顿悟**："反向建图后，输出层天然是拓扑起点，无需显式排序"
- **通用教训**：拓扑排序中入度清零判断需严格，避免漏处理层间依赖

---
处理用时：72.44秒
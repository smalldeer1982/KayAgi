# 题目信息

# [GCJ 2014 Finals] ARAM

## 题目背景

League of Legends 是 Riot Games 的商标。Riot Games 并未参与也未支持 Google Code Jam。


## 题目描述

在游戏 *League of Legends*（英雄联盟）中，有一种叫做 “ARAM”（All Random, All Mid，全随机单中路） 的游戏模式。本题借鉴了这一设定，但无需了解英雄联盟也能理解题意。

每次开始 ARAM 游戏时，你会从 $\mathrm{N}$ 个“英雄”（champions）中**等概率**地随机获得一个。你使用某些英雄时更容易获胜，因此当运气不好时，你可能希望自己能抽到另一个英雄。幸运的是，游戏中提供了“重新抽取”（Reroll）的功能。

重新抽取的机制如下所述，但你不能随时使用它。重新抽取的能力可以看作是一种货币：在你开始第一个 ARAM 游戏之前，你拥有 $\mathrm{R}$ 个“重新抽取点数”（RD，Reroll Dollars）。你只有在手上至少有 $1$ RD 时，才可以使用重新抽取，每次消耗 $1$ RD。

每打完一局游戏，你会获得 $1 / \mathrm{G}$ 个 RD（其中 $\mathrm{G}$ 是一个整数），但你的 RD 总数永远不会超过 $\mathrm{R}$：即使你已经拥有 $\mathrm{R}$ 个 RD，打完一局之后你仍然只有 $\mathrm{R}$ 个。

如果你手上有至少 $1$ RD，并选择使用重新抽取，则你会消耗 $1$ RD，并重新从 $\mathrm{N}$ 个英雄中**等概率**地抽取一个（可能会重复拿到当前的英雄）。如果你不满意新的英雄，并且还有 $1$ RD 以上，你可以继续重新抽取。只要你还剩下 RD，就可以继续抽。

例如，如果 $\mathrm{R} = 2$ 且 $\mathrm{G} = 2$，你在第一局游戏使用了一次重新抽取，那么该局之后你将拥有 $1.5$ RD。下一局若未使用重新抽取，该局结束后你将拥有 $2.0$ RD。再下一局若也未使用，那么仍为 $2.0$（因为不能超过上限）。如果你在接下来的游戏中使用了两次重新抽取，那么该局结束后你将剩下 $0.5$ RD。

你将得到一份英雄列表，以及每个英雄的胜率。如果你要打 $10^{100}$ 局游戏，并始终采用最优策略（即期望胜率最大化），那么你预期能赢下多少比例的游戏？


## 说明/提示

## 限制条件

- $1 \leq \mathrm{T} \leq 100$
- $0.0 \leq \mathrm{P}_i \leq 1.0$，每个胜率值格式为 1 位整数 + 小数点 + 4 位数字

### Small 数据集（22 分）

- 时间限制：~~60~~ 3 秒
- $1 \leq \mathrm{N} \leq 1000$
- $1 \leq \mathrm{R} \leq 2$
- $1 \leq \mathrm{G} \leq 3$

### Large 数据集（42 分）

- 时间限制：~~120~~ 5 秒
- $1 \leq \mathrm{N} \leq 1000$
- $1 \leq \mathrm{R} \leq 20$
- $1 \leq \mathrm{G} \leq 20$

翻译由 ChatGPT-4o 完成

## 样例 #1

### 输入

```
3
2 1 1
1.0000 0.0000
3 1 1
1.0000 0.0000 0.5000
6 2 3
0.9000 0.6000 0.5000 0.1000 0.2000 0.8000```

### 输出

```
Case #1: 0.750000000000
Case #2: 0.666666666667
Case #3: 0.618728522337```

# AI分析结果


### 💡 Kay的C++算法解析：ARAM 深入学习指南 💡

#### 1. 题目解读与核心算法识别
✨ **本题主要考察**：`动态规划`与`期望值分析`  
🗣️ **初步分析**：  
> 本题模拟ARAM游戏机制，核心在于**最优决策下的期望胜率最大化**。算法核心可比喻为"探险家寻宝"：  
> - 每次随机获得英雄（开宝箱），需决策是否消耗RD（金币）重抽  
> - 状态转移类似地图探索：当前RD量为坐标，决策影响后续"地图"（状态）和收益  
>  
> **核心难点**：  
> - 状态空间连续（RD为实数），需离散化为整数步长（1/G）  
> - 决策依赖当前状态和随机英雄胜率，需迭代求解期望值  
>  
> **可视化设计**：  
> 采用8位像素风RPG地图：  
> - 网格坐标 = RD状态（k值），英雄用不同颜色像素块表示  
> - 决策时高亮"宝箱开启"动画，接受/拒绝触发不同音效  
> - 自动演示模式展示状态转移（如`k→k-G`消耗RD，`k→k+1`结束回合）

---

#### 2. 精选优质题解参考
<eval_intro>  
暂无外部题解，Kay综合算法思想给出通用解法：  
> **核心策略**：离散化RD状态，迭代求解期望方程：  
> $$V(k) = \begin{cases} 
\frac{1}{N}\sum \max\begin{cases} 
p_i - E + V(\min(k+1, M)) \\
V(k-G) 
\end{cases} & k \geq G \\
P_{avg} - E + V(\min(k+1, M)) & k < G 
\end{cases}$$  
> 其中 $M = R \times G$，$E$ 为平均胜率，通过 $E = P_{avg} + V(1) - V(0)$ 迭代收敛

---

#### 3. 核心难点辨析与解题策略
1. **状态离散化**  
   * **分析**：RD连续值需转化为 $k = \lfloor r \cdot G \rfloor$（共 $R \times G + 1$ 状态），确保状态转移可计算  
   * 💡 学习笔记：连续问题离散化是DP常见技巧，步长选取依赖问题特性  

2. **期望方程构建**  
   * **分析**：  
     - 接受英雄：获 $p_i$ 胜率 + 转移到 $\min(k+1, M)$  
     - 拒绝英雄：无收益 + 转移到 $k-G$（消耗RD）  
     * 用 $p_i - E$ 消除无限局数发散性  
   * 💡 学习笔记：期望DP常引入相对值函数处理无限过程  

3. **迭代收敛控制**  
   * **分析**：  
     - 初始化 $E = P_{avg}$  
     - 每轮更新状态值 $V(k)$ 后，用边界状态 $V(0), V(1)$ 调整 $E$  
     - 收敛条件：$|E_{new} - E_{old}| < \epsilon$  
   * 💡 学习笔记：值迭代法求解非线性方程需谨慎设置阈值  

### ✨ 解题技巧总结
- **技巧1 状态压缩**：将浮点RD映射到整数 $k \in [0, R \times G]$  
- **技巧2 维度分离**：英雄胜率预处理为 $P_{avg}$，减少内层循环计算  
- **技巧3 剪枝优化**：当 $p_i - E + V_{\text{next}} < V(k-G)$ 时直接取拒绝值  

---

#### 4. C++核心代码实现赏析
**通用核心C++实现**  
```cpp
#include <iostream>
#include <vector>
#include <cmath>
#include <algorithm>
using namespace std;

const double EPS = 1e-9;

double solve(int N, int R, int G, vector<double>& probs) {
    double P_avg = 0.0;
    for (double p : probs) P_avg += p;
    P_avg /= N;

    int M = R * G + 1; // 状态数 k: 0 ~ R*G
    vector<double> V(M, 0.0);
    double E_prev = P_avg, E_next;

    for (int iter = 0; iter < 1000; ++iter) {
        vector<double> V_next(M, 0.0);
        for (int k = 0; k < M; ++k) {
            int next_accept = min(k + 1, M - 1);
            if (k < G) {
                V_next[k] = P_avg - E_prev + V[next_accept];
            } else {
                double total = 0.0;
                for (double p : probs) {
                    double accept_val = p - E_prev + V[next_accept];
                    double reject_val = V[k - G];
                    total += max(accept_val, reject_val);
                }
                V_next[k] = total / N;
            }
        }
        E_next = P_avg + V_next[1] - V_next[0];
        if (fabs(E_next - E_prev) < EPS) break;
        E_prev = E_next;
        V = V_next;
    }
    return E_next;
}
```
**代码解读概要**：  
1. 初始化平均胜率 $P_{avg}$ 和状态数组  
2. 迭代更新状态值 $V(k)$：  
   - $k < G$ 时强制接受英雄  
   - $k \geq G$ 时比较接受/拒绝收益  
3. 通过边界状态 $V(0), V(1)$ 更新期望胜率 $E$  

---

### 5. 算法可视化：像素动画演示
* **主题**：`像素勇者冒险`（8-bit风RPG地图）  
* **核心演示**：  
  ```mermaid
  graph LR
    A[开始局] --> B{有 RD？}
    B -->|是| C[随机英雄<br>宝箱开启动画]
    B -->|否| D[强制接受]
    C --> E{接受？}
    E -->|是| F[胜率计数<br>+1/G状态转移]
    E -->|否| G[消耗RD<br>k-G状态]
  ```

**关键帧设计**：  
1. **状态网格**：$x$ 轴为 $k$ 值（0 至 $R \times G$），不同 $k$ 用渐变颜色  
2. **决策特效**：  
   - 接受：英雄像素块爆炸 → 绿色`+p_i`飘字  
   - 拒绝：RD硬币消失 → 红色`-1`提示  
3. **音效系统**：  
   - 接受英雄：16-bit胜利音符  
   - 拒绝英雄：8-bit错误"哔"声  
   - 局结束：金币掉落音效  

**自动演示**：  
> 点击"AI运行"触发贪心决策路径：  
> - 高亮当前最优选择路径  
> - 实时显示期望值变化曲线  

---

#### 6. 拓展练习与相似问题
* **技巧迁移**：  
  1. 资源分配问题（如背包+再生资源）  
  2. 无限马尔可夫决策过程  
  3. 概率型游戏策略优化  

* **洛谷推荐**：  
  1. **P1853** 投资的最大效益  
     - 推荐理由：相似再生资源+无限期决策  
  2. **P4549** 裴蜀定理+资源分配  
  3. **P3211** 随机游走期望应用  

---

#### 7. 学习心得与经验分享
> **Kay的调试经验**：  
> - 浮点迭代需设阈值（如`1e-9`），避免无限循环  
> - 离散化时注意边界：$k = R \times G$ 对应 $r = R$  
> - 初始值 $E = P_{avg}$ 可加速收敛  

---

<conclusion>
通过本指南，我们深入解析了ARAM问题的期望DP解法。关键在于：**状态离散化建模**和**迭代求期望**。记住，算法如游戏——理解规则才能制定最优策略！下次遇到概率DP时，不妨想象成像素勇者的冒险之旅吧！🚀  
</conclusion>

---
处理用时：446.30秒
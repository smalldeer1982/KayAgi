# 题目信息

# [NOIP 2003 提高组] 神经网络

## 题目背景

人工神经网络（Artificial Neural Network）是一种新兴的具有自我学习能力的计算系统，在模式识别、函数逼近及贷款风险评估等诸多领域有广泛的应用。对神经网络的研究一直是当今的热门方向，兰兰同学在自学了一本神经网络的入门书籍后，提出了一个简化模型，他希望你能帮助他用程序检验这个神经网络模型的实用性。



## 题目描述

在兰兰的模型中，神经网络就是一张有向图，图中的节点称为神经元，而且两个神经元之间至多有一条边相连，下图是一个神经元的例子：

![](https://cdn.luogu.com.cn/upload/image_hosting/61qm40kj.png)

神经元（编号为 $i$）


图中，$X_1 \sim X_3$ 是信息输入渠道，$Y_1 \sim Y_2$ 是信息输出渠道，$C_i$ 表示神经元目前的状态，$U_i$ 是阈值，可视为神经元的一个内在参数。

神经元按一定的顺序排列，构成整个神经网络。在兰兰的模型之中，神经网络中的神经元分为几层；称为输入层、输出层，和若干个中间层。每层神经元只向下一层的神经元输出信息，只从上一层神经元接受信息。下图是一个简单的三层神经网络的例子。

![](https://cdn.luogu.com.cn/upload/image_hosting/4xd7f8yz.png)

兰兰规定，$C_i$ 服从公式：（其中 $n$ 是网络中所有神经元的数目）

$$C_i=\left(\sum\limits_{(j,i) \in E} W_{ji}C_{j}\right)-U_{i}$$

公式中的 $W_{ji}$（可能为负值）表示连接 $j$ 号神经元和 $i$ 号神经元的边的权值。当 $C_i$ 大于 $0$ 时，该神经元处于兴奋状态，否则就处于平静状态。当神经元处于兴奋状态时，下一秒它会向其他神经元传送信号，信号的强度为 $C_i$。

如此．在输入层神经元被激发之后，整个网络系统就在信息传输的推动下进行运作。现在，给定一个神经网络，及当前输入层神经元的状态（$C_i$），要求你的程序运算出最后网络输出层的状态。



## 说明/提示

**【题目来源】**

NOIP 2003 提高组第一题

## 样例 #1

### 输入

```
5 6
1 0
1 0
0 1
0 1
0 1
1 3 1
1 4 1
1 5 1
2 3 1
2 4 1
2 5 1
```

### 输出

```
3 1
4 1
5 1
```

# AI分析结果


```markdown
# 💡 Kay的C++算法解析：神经网络 深入学习指南 💡

## 1. 题目解读与核心算法识别

✨ **本题主要考察**：拓扑排序（DAG上的递推）

🗣️ **初步分析**：
> 解决"神经网络"这道题，关键在于理解并运用拓扑排序。简单来说，拓扑排序就像整理书架上的书籍，必须先把基础书籍放好才能在上面放更高级的书籍。在本题中，我们需要确保在计算神经元状态时，所有前驱神经元的状态已经计算完毕。

- 本题的核心在于分层递推计算神经元状态：只有状态>0的神经元才会传递信号，输入层（入度=0）不减去阈值，而非输入层在计算状态时需要减去一次阈值。
- 可视化方案将采用8位像素风格：神经元用彩色方块表示（绿色=激活态），信号传递过程用发光箭头动画展示，关键操作配以"叮"的音效。控制面板支持单步/自动播放，速度可调。

---

## 2. 精选优质题解参考

**题解一（Lucaster_）**
* **点评**：思路清晰直白，代码规范易读（变量名`hd`/`out`含义明确）。亮点在于巧妙处理阈值（输入层不减）和状态传递条件（c[i]>0才传递）。更新版代码优化了空间效率，实践价值高。

**题解二（zzlzk）**
* **点评**：对公式分析透彻（阈值U_i只需减一次），代码简洁高效。亮点在于用栈实现拓扑排序的创新方法，边界处理严谨（c[i]>0才输出），调试技巧实用。

**题解三（ghj1222）**
* **点评**：算法实现专业（标准拓扑队列），核心逻辑突出。亮点在于正确处理输入层阈值（直接忽略），状态转移条件判断精准，代码可直接用于竞赛场景。

---

## 3. 核心难点辨析与解题策略

1. **输入层与阈值处理**
   - **分析**：输入层神经元（初始c[i]>0）不减去阈值U_i，而非输入层需在状态计算前减去U_i（公式中独立于Σ的部分）
   - 💡 **学习笔记**：读入时通过`c[i]==0`判断非输入层直接减U_i

2. **状态传递条件**
   - **分析**：只有c[i]>0的神经元才会向后传递信号，这是避免无效计算的关键
   - 💡 **学习笔记**：拓扑排序中需显式判断`if(c[u] > 0)`再更新后继

3. **输出层识别与输出**
   - **分析**：出度=0的神经元为输出层，需按编号顺序输出c[i]>0的节点
   - 💡 **学习笔记**：遍历时记录出度，最终检查`out[i]==0 && c[i]>0`

### ✨ 解题技巧总结
- **预处理优化**：输入时直接处理非输入层阈值
- **拓扑排序应用**：用队列管理神经元处理顺序
- **边界鲁棒性**：显式处理NULL输出情况
- **调试技巧**：打印中间状态验证小规模样例

---

## 4. C++核心代码实现赏析

**本题通用核心C++实现参考**
```cpp
#include <iostream>
#include <queue>
#include <vector>
using namespace std;

const int N = 110;
int main() {
    // 初始化与输入处理（略）
    // 拓扑排序核心
    queue<int> q;
    for(int i=1; i<=n; i++) 
        if(in[i]==0) q.push(i);
    
    while(!q.empty()) {
        int u = q.front(); q.pop();
        if(c[u] > 0) // 关键状态判断
            for(auto [v,w] : G[u]) {
                c[v] += w * c[u];
                if(--in[v] == 0) // 更新入度
                    q.push(v);
            }
    }
    // 输出层处理（略）
}
```

**题解一核心片段**
```cpp
// 拓扑处理信号传递
while(!q.empty()) {
    h = q.front(); q.pop();
    for(i=hd[h]; i; i=e[i].next) {
        if(c[e[i].from] <= 0) continue; // 状态检查
        t = e[i].to;
        c[t] += e[i].val * c[h]; // 状态传递
        if(!vis[t]) q.push(t), vis[t]=1; // 入队控制
    }
}
```
**学习笔记**：前向星遍历时显式跳过非活跃神经元，避免无效计算

---

## 5. 算法可视化：像素动画演示

**设计概念**：
- 🎮 8位像素风格（FC红白机配色）
- 🧠 神经元：蓝色方块（静息态）→ 激活时变绿+闪烁
- ➡️ 信号传递：黄色脉冲沿边移动，伴随"叮"声
- 📊 实时数据显示：当前神经元状态/队列状态

**关键交互**：
1. **初始化**：绘制分层神经网络（输入层在上）
2. **拓扑执行**：
   - 队列中的神经元闪烁红光
   - 激活神经元触发脉冲动画（向所有出边发射）
   - 状态更新显示数值变化
3. **输出阶段**：输出层绿色神经元高亮+胜利音效

**技术实现**：Canvas绘制动态网格，requestAnimationFrame驱动动画，Web Audio API添加音效。

---

## 6. 拓展练习与相似问题思考

**通用技巧迁移**：
1. 任务调度（依赖关系拓扑排序）  
2. 关键路径分析（AOE网最长路径）  
3. 课程安排（LeetCode 210. Course Schedule II）

**推荐练习**：
1. 洛谷P1113：杂务（拓扑排序基础应用）
2. 洛谷P4017：最大食物链计数（拓扑+DP）
3. 洛谷P1983：车站分级（拓扑排序建模）

---

## 7. 学习心得与经验分享

> **Lucaster_的调试经验**：  
> "最初在`build`函数中冗余存储`from`字段，后发现队列中的`front`就是自然前驱节点。这提醒我们：解决问题后要反思设计是否过度复杂。"

**Kay总结**：  
调试图论问题时，建议：  
1. 打印每步的神经元状态变化  
2. 用迷你样例（如3节点）验证边界  
3. 可视化中间结果辅助理解数据流向
```

---
处理用时：148.65秒